{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataframes/full_data_clean_df_pickle4.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>NER</th>\n",
       "      <th>site</th>\n",
       "      <th>urls</th>\n",
       "      <th>bag_of_words</th>\n",
       "      <th>cleaned_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[brown sugar, milk, vanilla, nuts, butter, bit...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>http://www.cookbooks.com/Recipe-Details.aspx?i...</td>\n",
       "      <td>No-Bake Nut Cookies 1 c. firmly packed brown s...</td>\n",
       "      <td>no-bak nut cooki 1 firmli pack brown sugar 1/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[beef, chicken breasts, cream of mushroom soup...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>http://www.cookbooks.com/Recipe-Details.aspx?i...</td>\n",
       "      <td>Jewell Ball'S Chicken 1 small jar chipped beef...</td>\n",
       "      <td>jewel ball 's chicken 1 small jar chip beef cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[frozen corn, cream cheese, butter, garlic pow...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>http://www.cookbooks.com/Recipe-Details.aspx?i...</td>\n",
       "      <td>Creamy Corn 2 (16 oz.) pkg. frozen corn 1 (8 o...</td>\n",
       "      <td>creami corn 2 16 pkg frozen corn 1 8 pkg cream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[chicken, chicken gravy, cream of mushroom sou...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>http://www.cookbooks.com/Recipe-Details.aspx?i...</td>\n",
       "      <td>Chicken Funny 1 large whole chicken 2 (10 1/2 ...</td>\n",
       "      <td>chicken funni 1 larg whole chicken 2 10 1/2 ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[peanut butter, graham cracker crumbs, butter,...</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>http://www.cookbooks.com/Recipe-Details.aspx?i...</td>\n",
       "      <td>Reeses Cups(Candy)   1 c. peanut butter 3/4 c....</td>\n",
       "      <td>rees cup candi 1 peanut butter 3/4 graham crac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  title  \\\n",
       "0           0    No-Bake Nut Cookies   \n",
       "1           1  Jewell Ball'S Chicken   \n",
       "2           2            Creamy Corn   \n",
       "3           3          Chicken Funny   \n",
       "4           4   Reeses Cups(Candy)     \n",
       "\n",
       "                                                 NER               site  \\\n",
       "0  [brown sugar, milk, vanilla, nuts, butter, bit...  www.cookbooks.com   \n",
       "1  [beef, chicken breasts, cream of mushroom soup...  www.cookbooks.com   \n",
       "2  [frozen corn, cream cheese, butter, garlic pow...  www.cookbooks.com   \n",
       "3  [chicken, chicken gravy, cream of mushroom sou...  www.cookbooks.com   \n",
       "4  [peanut butter, graham cracker crumbs, butter,...  www.cookbooks.com   \n",
       "\n",
       "                                                urls  \\\n",
       "0  http://www.cookbooks.com/Recipe-Details.aspx?i...   \n",
       "1  http://www.cookbooks.com/Recipe-Details.aspx?i...   \n",
       "2  http://www.cookbooks.com/Recipe-Details.aspx?i...   \n",
       "3  http://www.cookbooks.com/Recipe-Details.aspx?i...   \n",
       "4  http://www.cookbooks.com/Recipe-Details.aspx?i...   \n",
       "\n",
       "                                        bag_of_words  \\\n",
       "0  No-Bake Nut Cookies 1 c. firmly packed brown s...   \n",
       "1  Jewell Ball'S Chicken 1 small jar chipped beef...   \n",
       "2  Creamy Corn 2 (16 oz.) pkg. frozen corn 1 (8 o...   \n",
       "3  Chicken Funny 1 large whole chicken 2 (10 1/2 ...   \n",
       "4  Reeses Cups(Candy)   1 c. peanut butter 3/4 c....   \n",
       "\n",
       "                                         cleaned_bow  \n",
       "0  no-bak nut cooki 1 firmli pack brown sugar 1/2...  \n",
       "1  jewel ball 's chicken 1 small jar chip beef cu...  \n",
       "2  creami corn 2 16 pkg frozen corn 1 8 pkg cream...  \n",
       "3  chicken funni 1 larg whole chicken 2 10 1/2 ca...  \n",
       "4  rees cup candi 1 peanut butter 3/4 graham crac...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1643098 entries, 0 to 1643097\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count    Dtype \n",
      "---  ------        --------------    ----- \n",
      " 0   Unnamed: 0    1643098 non-null  int64 \n",
      " 1   title         1643098 non-null  object\n",
      " 2   NER           1643098 non-null  object\n",
      " 3   site          1643098 non-null  object\n",
      " 4   urls          1643098 non-null  object\n",
      " 5   bag_of_words  1643098 non-null  object\n",
      " 6   cleaned_bow   1643098 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 100.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df.sample(frac=0.01, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_cleaned = df_subset['cleaned_bow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/coxem/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LatentDirichletAllocation from version 0.24.0 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "lda_model_1 = joblib.load('./models/lda_model.joblib')\n",
    "lda_model_2 = joblib.load('./models/lda_model_1_tid.joblib')\n",
    "lda_model_3 = joblib.load('./models/lda_model_2_tdn.joblib')\n",
    "lda_model_4 = joblib.load('./models/lda_model_2_tid.joblib')\n",
    "lda_model_5 = joblib.load('./models/lda_model_3_tid.joblib')\n",
    "lda_model_6 = joblib.load('./models/lda_model_4_tid.joblib')\n",
    "lda_model_7 = joblib.load('./models/lda_model_5_tid.joblib')\n",
    "lda_model_8 = joblib.load('./models/lda_model_6_tid.joblib')\n",
    "lda_model_8 = joblib.load('./models/lda_model_6_tid_pickle4.joblib')\n",
    "lda_model_9 = joblib.load('./models/lda_model_full_tid_pickle4.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [lda_model_1, lda_model_2, lda_model_3, lda_model_4, lda_model_5, lda_model_6, lda_model_7, lda_model_8, lda_model_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'doc_topic_prior': None,\n",
       " 'evaluate_every': -1,\n",
       " 'learning_decay': 0.7,\n",
       " 'learning_method': 'online',\n",
       " 'learning_offset': 10.0,\n",
       " 'max_doc_update_iter': 100,\n",
       " 'max_iter': 10,\n",
       " 'mean_change_tol': 0.001,\n",
       " 'n_components': 30,\n",
       " 'n_jobs': -1,\n",
       " 'perp_tol': 0.1,\n",
       " 'random_state': None,\n",
       " 'topic_word_prior': None,\n",
       " 'total_samples': 1000000.0,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 1\n",
      "\n",
      "Parameters:{'batch_size': 128, 'doc_topic_prior': None, 'evaluate_every': -1, 'learning_decay': 0.7, 'learning_method': 'online', 'learning_offset': 10.0, 'max_doc_update_iter': 100, 'max_iter': 10, 'mean_change_tol': 0.001, 'n_components': 30, 'n_jobs': -1, 'perp_tol': 0.1, 'random_state': None, 'topic_word_prior': None, 'total_samples': 1000000.0, 'verbose': 0}\n",
      "\n",
      "Model: 2\n",
      "\n",
      "Parameters:{'batch_size': 128, 'doc_topic_prior': None, 'evaluate_every': -1, 'learning_decay': 0.7, 'learning_method': 'online', 'learning_offset': 10.0, 'max_doc_update_iter': 100, 'max_iter': 10, 'mean_change_tol': 0.001, 'n_components': 30, 'n_jobs': -1, 'perp_tol': 0.1, 'random_state': None, 'topic_word_prior': None, 'total_samples': 1000000.0, 'verbose': 0}\n",
      "\n",
      "Model: 3\n",
      "\n",
      "Parameters:{'batch_size': 128, 'doc_topic_prior': None, 'evaluate_every': -1, 'learning_decay': 0.7, 'learning_method': 'online', 'learning_offset': 10.0, 'max_doc_update_iter': 100, 'max_iter': 10, 'mean_change_tol': 0.001, 'n_components': 30, 'n_jobs': -1, 'perp_tol': 0.1, 'random_state': None, 'topic_word_prior': None, 'total_samples': 1000000.0, 'verbose': 0}\n",
      "\n",
      "Model: 4\n",
      "\n",
      "Parameters:{'batch_size': 128, 'doc_topic_prior': None, 'evaluate_every': -1, 'learning_decay': 0.7, 'learning_method': 'online', 'learning_offset': 10.0, 'max_doc_update_iter': 100, 'max_iter': 10, 'mean_change_tol': 0.001, 'n_components': 30, 'n_jobs': -1, 'perp_tol': 0.1, 'random_state': None, 'topic_word_prior': None, 'total_samples': 1000000.0, 'verbose': 0}\n",
      "\n",
      "Model: 5\n",
      "\n",
      "Parameters:{'batch_size': 128, 'doc_topic_prior': None, 'evaluate_every': -1, 'learning_decay': 0.7, 'learning_method': 'online', 'learning_offset': 10.0, 'max_doc_update_iter': 100, 'max_iter': 10, 'mean_change_tol': 0.001, 'n_components': 100, 'n_jobs': -1, 'perp_tol': 0.1, 'random_state': None, 'topic_word_prior': None, 'total_samples': 1000000.0, 'verbose': 0}\n",
      "\n",
      "Model: 6\n",
      "\n",
      "Parameters:{'batch_size': 32, 'doc_topic_prior': None, 'evaluate_every': -1, 'learning_decay': 0.7, 'learning_method': 'online', 'learning_offset': 20, 'max_doc_update_iter': 100, 'max_iter': 10, 'mean_change_tol': 0.001, 'n_components': 100, 'n_jobs': -1, 'perp_tol': 0.1, 'random_state': None, 'topic_word_prior': None, 'total_samples': 1000000.0, 'verbose': 0}\n",
      "\n",
      "Model: 7\n",
      "\n",
      "Parameters:{'batch_size': 32, 'doc_topic_prior': 0.9, 'evaluate_every': -1, 'learning_decay': 0.7, 'learning_method': 'online', 'learning_offset': 20, 'max_doc_update_iter': 100, 'max_iter': 10, 'mean_change_tol': 0.001, 'n_components': 100, 'n_jobs': -1, 'perp_tol': 0.1, 'random_state': None, 'topic_word_prior': 0.9, 'total_samples': 1000000.0, 'verbose': 0}\n",
      "\n",
      "Model: 8\n",
      "\n",
      "Parameters:{'batch_size': 32, 'doc_topic_prior': 0.9, 'evaluate_every': -1, 'learning_decay': 0.7, 'learning_method': 'online', 'learning_offset': 50, 'max_doc_update_iter': 100, 'max_iter': 10, 'mean_change_tol': 0.001, 'n_components': 100, 'n_jobs': -1, 'perp_tol': 0.1, 'random_state': None, 'topic_word_prior': 0.9, 'total_samples': 1000000.0, 'verbose': 0}\n",
      "\n",
      "Model: 9\n",
      "\n",
      "Parameters:{'batch_size': 32, 'doc_topic_prior': 0.9, 'evaluate_every': -1, 'learning_decay': 0.7, 'learning_method': 'online', 'learning_offset': 50, 'max_doc_update_iter': 100, 'max_iter': 10, 'mean_change_tol': 0.001, 'n_components': 100, 'n_jobs': -1, 'perp_tol': 0.1, 'random_state': None, 'topic_word_prior': 0.9, 'total_samples': 1000000.0, 'verbose': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for model in models:\n",
    "    i += 1\n",
    "    print(f'Model: {i}\\n\\nParameters:{model.get_params()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models 1 - 4 are the same: n_components =30, everything else default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 5 changed n_components to 100, everything else default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 6 changed batch size to 32, learning offset 20, everything else the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 7 doc_topic_prior 0.9 topic_word_prior 0.9, everything else the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 8 learning offset to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/coxem/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.24.0 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "vec_1 = joblib.load('./models/vec.joblib')\n",
    "vec_2 = joblib.load('./models/vec_1_tid.joblib')\n",
    "vec_3 = joblib.load('./models/vec_2_tdn.joblib')\n",
    "vec_4 = joblib.load('./models/vec_2_tid.joblib')\n",
    "vec_5 = joblib.load('./models/vec_3_tid.joblib')\n",
    "vec_6 = joblib.load('./models/vec_4_tid.joblib')\n",
    "vec_7 = joblib.load('./models/vec_5_tid.joblib')\n",
    "vec_8 = joblib.load('./models/vec_6_tid.joblib')\n",
    "vec_9 = joblib.load('./models/vec_6_tid_pickle4.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = [vec_1, vec_2, vec_3, vec_4, vec_5, vec_6, vec_7, vec_8, vec_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer: 1\n",
      "\n",
      "Parameters:{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.85, 'max_features': 1000, 'min_df': 2, 'ngram_range': (1, 2), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n",
      "\n",
      "Vectorizer: 2\n",
      "\n",
      "Parameters:{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.85, 'max_features': 1000, 'min_df': 2, 'ngram_range': (1, 2), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n",
      "\n",
      "Vectorizer: 3\n",
      "\n",
      "Parameters:{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.85, 'max_features': 1000, 'min_df': 2, 'ngram_range': (1, 3), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n",
      "\n",
      "Vectorizer: 4\n",
      "\n",
      "Parameters:{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.85, 'max_features': 1000, 'min_df': 2, 'ngram_range': (1, 3), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n",
      "\n",
      "Vectorizer: 5\n",
      "\n",
      "Parameters:{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.85, 'max_features': 1000, 'min_df': 2, 'ngram_range': (1, 3), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n",
      "\n",
      "Vectorizer: 6\n",
      "\n",
      "Parameters:{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.85, 'max_features': 1000, 'min_df': 2, 'ngram_range': (1, 3), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n",
      "\n",
      "Vectorizer: 7\n",
      "\n",
      "Parameters:{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.85, 'max_features': 500, 'min_df': 2, 'ngram_range': (1, 2), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n",
      "\n",
      "Vectorizer: 8\n",
      "\n",
      "Parameters:{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.85, 'max_features': 500, 'min_df': 2, 'ngram_range': (1, 2), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n",
      "\n",
      "Vectorizer: 9\n",
      "\n",
      "Parameters:{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.85, 'max_features': 500, 'min_df': 2, 'ngram_range': (1, 2), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for vec in vecs:\n",
    "    i += 1\n",
    "    print(f'Vectorizer: {i}\\n\\nParameters:{vec.get_params()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizer 1 and 2: max_df=0.85, min_df=2, max_features=1000, n_gram_range=(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizer 3, 4, 5, 6: n_gram_range=(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizer 7, 8, 9: max_features = 500, n_gram_range = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_10 = CountVectorizer(max_df=0.85, \n",
    "                      min_df=10,\n",
    "                      ngram_range=(1,3),\n",
    "                      max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizer 10: min_df=10, ngram_range=(1,3), max_features=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec1 = vec_1.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
