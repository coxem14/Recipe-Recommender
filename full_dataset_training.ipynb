{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer as PS\n",
    "\n",
    "import joblib\n",
    "\n",
    "import pickle\n",
    "\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('../full_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.loc[data['source'] == 'Gathered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.drop(columns='source', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def literal_return(val):\n",
    "#     try:\n",
    "#         return literal_eval(val)\n",
    "#     except (ValueError, SyntaxError) as e:\n",
    "#         return val\n",
    "\n",
    "# def df_str_to_literal(df, column_name):\n",
    "#     df[column_name] = df[column_name].apply(literal_return)\n",
    "#     return df[column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_df(df, columns_list):\n",
    "#     for col in columns_list:\n",
    "#         df[col] = df_str_to_literal(df, col)\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df(data, ['ingredients', 'directions', 'NER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['bag_of_words'] = ''\n",
    "# columns_list = ['title', 'ingredients', 'directions']\n",
    "# for col in columns_list:\n",
    "#     if col == 'title':\n",
    "#         data['bag_of_words'] += data[col] + ' '\n",
    "#     if col == 'ingredients':\n",
    "#         data['bag_of_words'] += data[col].apply(' '.join) + ' '\n",
    "#     if col == 'directions':\n",
    "#         data['bag_of_words'] += data[col].apply(' '.join)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>bag_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>No-Bake Nut Cookies 1 c. firmly packed brown s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>Jewell Ball'S Chicken 1 small jar chipped beef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>Creamy Corn 2 (16 oz.) pkg. frozen corn 1 (8 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>Chicken Funny 1 large whole chicken 2 (10 1/2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>Reeses Cups(Candy)   1 c. peanut butter 3/4 c....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                       bag_of_words\n",
       "0    No-Bake Nut Cookies  No-Bake Nut Cookies 1 c. firmly packed brown s...\n",
       "1  Jewell Ball'S Chicken  Jewell Ball'S Chicken 1 small jar chipped beef...\n",
       "2            Creamy Corn  Creamy Corn 2 (16 oz.) pkg. frozen corn 1 (8 o...\n",
       "3          Chicken Funny  Chicken Funny 1 large whole chicken 2 (10 1/2 ...\n",
       "4   Reeses Cups(Candy)    Reeses Cups(Candy)   1 c. peanut butter 3/4 c...."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow_data = data[['title','bag_of_words']]\n",
    "# bow_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = bow_data.bag_of_words\n",
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_document(document):\n",
    "#     '''\n",
    "#     Takes in a string.\n",
    "#     Returns cleaned string.\n",
    "#     '''\n",
    "#     # lowercase the strings\n",
    "#     doc_lower = document.lower() \n",
    "\n",
    "#     #tokenize\n",
    "#     tokens = word_tokenize(doc_lower) \n",
    "    \n",
    "#     # remove punctuation\n",
    "#     punc = set(string.punctuation)\n",
    "#     tokens_no_punc = [word for word in tokens if word not in punc]\n",
    "   \n",
    "#     # remove stopwords\n",
    "#     s_words = set(stopwords.words('english'))\n",
    "#     s_words_list = ['tablespoon', 'tbsp', 'teaspoon', 'tsp', 'cup', 'oz', 'lb', 'c.']\n",
    "#     for word in s_words_list:\n",
    "#         s_words.add(word)\n",
    "#     tokens_no_sw = [word for word in tokens_no_punc if word not in s_words]\n",
    "    \n",
    "#     # stem the words to get rid of multiple forms of the same word\n",
    "#     porter = PS()\n",
    "#     tokens_stemmed = [porter.stem(word) for word in tokens_no_sw]\n",
    "    \n",
    "#     # join all words into one string\n",
    "#     cleaned_doc = ' '.join(tokens_stemmed)\n",
    "    \n",
    "#     return cleaned_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_data['cleaned_bow'] = bow_data['bag_of_words'].apply(clean_document)\n",
    "# bow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle('./models/full_data_df_pickle_4.pkl', protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle('./models/full_data_df_pickle_5.pkl', protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_data.to_pickle('./models/training_df_pickle_4.pkl', protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_data.to_pickle('./models/training_df_pickle_5.pkl', protocol=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 1000\n",
    "ngram_range=(1,3)\n",
    "\n",
    "vec = CountVectorizer(max_df=0.85, \n",
    "                      min_df=10,\n",
    "                      ngram_range=ngram_range,\n",
    "                      max_features=num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/full_vec_pickle4.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vec, './models/full_vec_pickle4.joblib', protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/full_vec_pickle5.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vec, './models/full_vec_pickle5.joblib', protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/training_df_pickle_4.pkl', 'rb') as f:\n",
    "    bow_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_cleaned = bow_data['cleaned_bow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = vec.fit_transform(docs_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_feature_names = vec.get_feature_names()\n",
    "# tf_feature_names[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_topics = [100]\n",
    "# learning_method=['online']\n",
    "# learning_offset = [10, 50, 90]\n",
    "# doc_topic_prior = [None, 0.1, 0.9]\n",
    "# topic_word_prior = [None, 0.1, 0.9]\n",
    "# learning_decay = [0.5, 0.7, 0.9]\n",
    "# batch_size = [64, 128]\n",
    "# n_jobs= [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_grid = {'n_components': num_topics,\n",
    "#                'learning_method':learning_method,\n",
    "#                'learning_offset': learning_offset,\n",
    "#                'doc_topic_prior': doc_topic_prior,\n",
    "#                'topic_word_prior': topic_word_prior,\n",
    "#                'learning_decay': learning_decay,\n",
    "#                'batch_size': batch_size,\n",
    "#                'n_jobs': n_jobs}\n",
    "\n",
    "# pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda = LatentDirichletAllocation()\n",
    "\n",
    "# lda_grid = RandomizedSearchCV(estimator=lda, \n",
    "#                               param_distributions=random_grid,\n",
    "#                               cv=5,\n",
    "#                               n_iter=10,\n",
    "#                               n_jobs=-1, \n",
    "#                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_grid.fit(tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train, tf_test = train_test_split(tf, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = joblib.load('./models/lda_model_6_tid_pickle4.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'doc_topic_prior': 0.9,\n",
       " 'evaluate_every': -1,\n",
       " 'learning_decay': 0.7,\n",
       " 'learning_method': 'online',\n",
       " 'learning_offset': 50,\n",
       " 'max_doc_update_iter': 100,\n",
       " 'max_iter': 10,\n",
       " 'mean_change_tol': 0.001,\n",
       " 'n_components': 100,\n",
       " 'n_jobs': -1,\n",
       " 'perp_tol': 0.1,\n",
       " 'random_state': None,\n",
       " 'topic_word_prior': 0.9,\n",
       " 'total_samples': 1000000.0,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=32, doc_topic_prior=0.9,\n",
       "                          learning_method='online', learning_offset=50,\n",
       "                          n_components=100, n_jobs=-1, topic_word_prior=0.9)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: -195067790.35154256\n"
     ]
    }
   ],
   "source": [
    "print('Test Score:', lda.score(tf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score -584607435.4425652\n"
     ]
    }
   ],
   "source": [
    "print('Train Score', lda.score(tf_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 398.5728608682162\n"
     ]
    }
   ],
   "source": [
    "print('Test Perplexity:', lda.perplexity(tf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexity: 393.9414662767103\n"
     ]
    }
   ],
   "source": [
    "print('Train Perplexity:', lda.perplexity(tf_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/lda_model_full_tid_pickle4.joblib']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lda, './models/lda_model_full_tid_pickle4.joblib', protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame.from_dict(lda_grid.cv_results_))\n",
    "# print('Test Score:', lda_grid.score(tf_test))\n",
    "# print('Perplexity:', lda_grid.perplexity(tf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_lda_model = lda_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(best_lda_model, './models/lda_model_full_tid_pickle4.joblib', protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(best_lda_model, './models/lda_model_full_tid_pickle5.joblib', protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(vec, './models/vec_full_tid_pickle4.joblib', protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(vec, './models/vec_full_tid_pickle5.joblib', protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best Model's Params: \", lda_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best Model's Log Likelihood Score: \", lda_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best Model's Perplexity: \", best_lda_model.perplexity(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "minut 20 20 minut 15 20 18 15 20 minut full bake almost medium\n",
      "Topic 1:\n",
      "flour flour salt add flour flour mixtur sugar flour gradual combin flour gradual add togeth flour slowli\n",
      "Topic 2:\n",
      "water cold boil water dissolv cold water cup water gelatin soak add water water add\n",
      "Topic 3:\n",
      "boil bring pot bring boil simmer liquid salt water water boil larg pot boil add\n",
      "Topic 4:\n",
      "orang crush pineappl banana fruit cherri orang juic crush pineappl jello pear\n",
      "Topic 5:\n",
      "vinegar jar bag cucumb wash marinad qt lid seal pickl\n",
      "Topic 6:\n",
      "peel appl toss drizzl caramel medium cider tart quarter tender\n",
      "Topic 7:\n",
      "combin bowl larg bowl 25 bowl combin 25 minut 20 25 larg bowl combin in bowl stir\n",
      "Topic 8:\n",
      "whisk rack center complet batter clean come bowl larg egg wire\n",
      "Topic 9:\n",
      "fresh parsley thyme paprika chop fresh herb rosemari sprig chive fresh parsley\n",
      "Topic 10:\n",
      "place half anoth repeat and bottom almost bowl side rest\n",
      "Topic 11:\n",
      "pour hour option flavor 325 300 bake hour altern bake firm\n",
      "Topic 12:\n",
      "tablespoon tablespoon butter creami sugar tablespoon add butter lb minut medium almost bowl\n",
      "Topic 13:\n",
      "top sprinkl spread bottom top bake evenli bake almost minut doubl\n",
      "Topic 14:\n",
      "side fill divid evenli plate press gentli edg plu around\n",
      "Topic 15:\n",
      "bake soda bake powder bake soda flour bake sift shorten muffin flour bake powder soda salt\n",
      "Topic 16:\n",
      "heat low saucepan remov heat thicken cornstarch low heat heat stir constantli heat add\n",
      "Topic 17:\n",
      "medium skillet heat pound fat medium heat turkey skillet medium larg skillet per\n",
      "Topic 18:\n",
      "butter melt melt butter butter melt add butter butter margarin butter sugar sugar butter bake cream butter\n",
      "Topic 19:\n",
      "pie whip crust cracker shell whip cream cool whip graham graham cracker blueberri\n",
      "Topic 20:\n",
      "drain corn can quart rins minut drain tender onion uncook add\n",
      "Topic 21:\n",
      "stick soft size enough continu add bowl add stick butter approxim slowli\n",
      "Topic 22:\n",
      "minut 15 15 minut 10 15 almost bake medium add 425 heat\n",
      "Topic 23:\n",
      "vanilla nut coconut sugar egg oat butter sugar egg vanilla oleo sugar vanilla chop nut\n",
      "Topic 24:\n",
      "oven preheat preheat oven degre oven 350 bake preheat oven 350 350 degre heat oven oven 350 degre\n",
      "Topic 25:\n",
      "whole honey halv cranberri yogurt plain wheat bowl berri whole wheat\n",
      "Topic 26:\n",
      "smooth food transfer blender second pure processor food processor color unsalt\n",
      "Topic 27:\n",
      "leav seed garnish cilantro cumin sesam dill jalapeno curri chile\n",
      "Topic 28:\n",
      "salt garlic salt add salt salt water pepper minut tender almost add medium\n",
      "Topic 29:\n",
      "thick steak zucchini strip round squash brush yellow inch thick pizza\n",
      "Topic 30:\n",
      "well mix well mix well add stir well beat well well blend ingredi mix add egg blend\n",
      "Topic 31:\n",
      "roll dough ball purpos purpos flour all all purpos all purpos flour yeast shape\n",
      "Topic 32:\n",
      "minut 30 30 minut uncov minut serv 350 30 bake 25 in almost\n",
      "Topic 33:\n",
      "stir occasion minut stir cook stir stir occasion stir togeth frequent almost bowl stir minut\n",
      "Topic 34:\n",
      "one time littl two like allow hand take bit start\n",
      "Topic 35:\n",
      "make pinch 375 bake drop pinch salt bake 375 oven 375 salt sugar full\n",
      "Topic 36:\n",
      "cooki sheet bake temperatur bake sheet room room temperatur line cooki sheet pastri\n",
      "Topic 37:\n",
      "mixtur bowl separ mix bowl bowl mix mixtur stir bowl stir firm measur fold\n",
      "Topic 38:\n",
      "10 minut 10 minut bake 10 almost bake medium add firm heat\n",
      "Topic 39:\n",
      "lightli coat prepar spray cook spray next lightli brown bowl stir nonstick fre\n",
      "Topic 40:\n",
      "cake 13 strawberri pud box 13 inch frost instant cake mix inch pan\n",
      "Topic 41:\n",
      "small cube small bowl amount small onion bouillon small piec bowl almost tender\n",
      "Topic 42:\n",
      "powder powder sugar 40 garlic powder 35 40 minut powder salt 35 minut bake minut\n",
      "Topic 43:\n",
      "tomato bean chili past tortilla tomato sauc chili powder hamburg salsa taco\n",
      "Topic 44:\n",
      "chicken rice broth breast soy chicken breast soy sauc chicken broth boneless add chicken\n",
      "Topic 45:\n",
      "set asid set asid medium reserv medium bowl bowl almost minut dark\n",
      "Topic 46:\n",
      "bake 45 45 minut buttermilk sugar salt flour sugar peach biscuit 50 60\n",
      "Topic 47:\n",
      "garlic clove minc garlic clove clove garlic oregano onion garlic clove minc garlic clove minc garlic minc\n",
      "Topic 48:\n",
      "milk 400 evapor egg milk add milk oven 400 evapor milk milk egg preheat oven 400 bake 400\n",
      "Topic 49:\n",
      "brown golden golden brown minut golden cornmeal bake minut brown sugar almost in\n",
      "Topic 50:\n",
      "hot add bubbl long hot water slowli steam pan add serv hot grain\n",
      "Topic 51:\n",
      "fri min paper coars towel none wedg meanwhil deep paper towel\n",
      "Topic 52:\n",
      "ground ginger almond toast walnut spice pumpkin mapl unsweeten ground cinnamon\n",
      "Topic 53:\n",
      "green pepper onion pork green onion green pepper bell bell pepper chop green pepper chop\n",
      "Topic 54:\n",
      "togeth mix mix togeth mix ingredi ingredi togeth mix ingredi togeth stir togeth togeth flour bake minut\n",
      "Topic 55:\n",
      "remov foil minut remov minut skin longer return discard 425 minut longer\n",
      "Topic 56:\n",
      "onion dice celeri carrot pea onion chop medium onion add onion tender larg onion\n",
      "Topic 57:\n",
      "cinnamon brown sugar sugar light nutmeg raisin pack ground cinnamon sugar butter sugar cinnamon\n",
      "Topic 58:\n",
      "salt pepper pepper tast season pepper tast salt pepper tast season salt cabbag season salt pepper salt tast\n",
      "Topic 59:\n",
      "oil veget veget oil heat oil oil larg oil medium till heat tender medium\n",
      "Topic 60:\n",
      "add minut add add salt almost heat add add onion minut slowli simmer saut\n",
      "Topic 61:\n",
      "black black pepper roast pepper grill fish ground black freshli ground black pepper kosher\n",
      "Topic 62:\n",
      "cut piec cut inch chunk bite inch piec in lb quarter altern\n",
      "Topic 63:\n",
      "slice thinli thin thinli slice arrang onion slice sandwich ripe platter tender\n",
      "Topic 64:\n",
      "egg margarin beaten butter margarin egg beaten beaten egg add egg stick margarin egg sugar egg mixtur\n",
      "Topic 65:\n",
      "cup teaspoon cup sugar cup flour teaspoon salt teaspoon bake cup water full measur bowl\n",
      "Topic 66:\n",
      "larg desir larg onion larg bowl larg egg almost bowl minut medium add\n",
      "Topic 67:\n",
      "ingredi first combin ingredi except first ingredi ingredi except easi last yield next\n",
      "Topic 68:\n",
      "salad bacon dress mayonnais crumbl crisp lettuc head cauliflow hard\n",
      "Topic 69:\n",
      "ice extract syrup vanilla vanilla extract glass ice cream granul freez granul sugar\n",
      "Topic 70:\n",
      "pan pan bake bake pan remov pan pan bake 350 greas flour bottom bake pan add almost\n",
      "Topic 71:\n",
      "soup mushroom casserol broccoli stuf mushroom soup casserol dish chicken soup cream mushroom cream chicken\n",
      "Topic 72:\n",
      "bread crumb dip loaf bread crumb french shallow loaf pan shape bake\n",
      "Topic 73:\n",
      "cook cook minut cover cook tender medium almost cook low medium onion cook medium heat cook\n",
      "Topic 74:\n",
      "ounc packag 16 14 ounc packag 10 ounc 14 ounc 12 ounc 16 ounc 15 ounc\n",
      "Topic 75:\n",
      "beat white egg egg white yolk mixer beat egg egg yolk speed white sugar\n",
      "Topic 76:\n",
      "dri greas dri ingredi thoroughli bake minut pat altern add full\n",
      "Topic 77:\n",
      "juic lemon lemon juic lime zest lime juic mint rind lemon zest squeez\n",
      "Topic 78:\n",
      "chees shred cheddar cheddar chees macaroni chees melt sharp shred cheddar grate chees chees bake\n",
      "Topic 79:\n",
      "pkg cream chees pecan soften chop pecan chees soften pkg cream pkg cream chees cream chees soften butter soften\n",
      "Topic 80:\n",
      "bake 350 dish bake 350 bake dish inch bake minut 350 350 hour 13 top bake\n",
      "Topic 81:\n",
      "refriger chill hour wrap contain least overnight plastic store day\n",
      "Topic 82:\n",
      "use keep need recip may ad readi good work note\n",
      "Topic 83:\n",
      "sauc beef meat ground beef worcestershir worcestershir sauc slow ketchup meatbal lamb\n",
      "Topic 84:\n",
      "oliv oliv oil oil wine extra tablespoon oliv tablespoon oliv oil shallot white wine virgin\n",
      "Topic 85:\n",
      "layer frozen noodl spinach thaw mozzarella mozzarella chees cottag babi cottag chees\n",
      "Topic 86:\n",
      "spoon turn warm slightli addit immedi begin break serv immedi wooden\n",
      "Topic 87:\n",
      "mix put done rest almost mix bowl bake full add bowl\n",
      "Topic 88:\n",
      "serv cover quarter platter cover simmer almost tender lb medium hour\n",
      "Topic 89:\n",
      "sugar blend add sugar sugar add dissolv white sugar butter sugar sugar egg sugar butter altern\n",
      "Topic 90:\n",
      "grate parmesan pasta basil parmesan chees italian direct accord spaghetti grate parmesan\n",
      "Topic 91:\n",
      "chocol chip peanut chocol chip peanut butter cocoa microwav candi confection confection sugar\n",
      "Topic 92:\n",
      "heat high reduc high heat reduc heat simmer medium high stock medium medium high heat\n",
      "Topic 93:\n",
      "potato fine sweet fine chop sausag mash sweet potato onion fine tender lb\n",
      "Topic 94:\n",
      "inch 12 squar fork 12 minut 11 10 12 pat firm minut\n",
      "Topic 95:\n",
      "cream sour sour cream heavi heavi cream cream butter whip cream fold blend altern\n",
      "Topic 96:\n",
      "remain mustard remain ingredi dash add remain add remain ingredi dijon add divid blend\n",
      "Topic 97:\n",
      "chop chop onion ham onion chop chop celeri onion bone tender uncook pepper\n",
      "Topic 98:\n",
      "pepper red saut shrimp flake red pepper cayenn red onion saut onion cayenn pepper\n",
      "Topic 99:\n",
      "cool let stand let stand let cool minut cool sit almost minut firm\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 10\n",
    "tf_feature_names = vec.get_feature_names()\n",
    "display_topics(lda, tf_feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_recipes(keyword, recipes, probs, n_recipes=10):\n",
    "    idx_arr = np.array(recipes.index)\n",
    "    keyword_recipes = recipes[recipes.str.contains(keyword, case=False, regex=False)]\n",
    "    keyword_samples = np.random.choice(keyword_recipes.index, size=50, replace=True)\n",
    "    keyword_idxs = []\n",
    "    for sample_idx in keyword_samples:\n",
    "        keyword_idx = int(np.where(idx_arr == sample_idx)[0])\n",
    "        keyword_idxs.append(keyword_idx)\n",
    "\n",
    "    d={}\n",
    "    for idx in keyword_idxs:\n",
    "        sims = cosine_distances(probs[idx].reshape(1, -1), probs).argsort()[0]\n",
    "        for sim in sims[1:n_recipes+1]:\n",
    "            if sim not in d:\n",
    "                d[sim] = 1\n",
    "            else:\n",
    "                d[sim] += 1\n",
    "                \n",
    "    d_sorted = [k for k, v in sorted(d.items(), key=lambda item: item[1])][:-n_recipes:-1]\n",
    "    print(f'Top {n_recipes} recipes most closely related to {keyword}')\n",
    "    return np.array(recipes)[d_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = bow_data.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recipes most closely related to bean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Red Beans And Rice', 'Calico Bean Pot', 'Championship Elk Chili',\n",
       "       'Frijoles', 'Cowpoke Beans ',\n",
       "       'Habichuelas Rojas Guisadas (Stewed Red Beans) ',\n",
       "       'Vegetarian Chili (Good For Non-Spicy Lovers)',\n",
       "       'Winter White Chili', 'Stewed Tomato Soup'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_recipes('bean', recipes, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          No-Bake Nut Cookies\n",
       "1                        Jewell Ball'S Chicken\n",
       "2                                  Creamy Corn\n",
       "3                                Chicken Funny\n",
       "4                         Reeses Cups(Candy)  \n",
       "                          ...                 \n",
       "1643093             Tuna 'N Egg Salad In Pitas\n",
       "1643094                 Croque Monsieur Panini\n",
       "1643095    Croque Monsieur With Cucumber Salad\n",
       "1643096                       Baked Pork Chops\n",
       "1643097            Date Filled Oatmeal Cookies\n",
       "Name: title, Length: 1643098, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_recipes('vegan', recipes, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_recipes('pepperoni pizza', recipes, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_recipes('spaghetti', recipes, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recipes most closely related to chicken soup\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Broccoli Soup', 'Mushroom Soup', 'Beef And Chicken Soup',\n",
       "       'Best Broccoli Soup', 'Chicken Stew', 'Chicken Soup',\n",
       "       'Quickie Tom Yum Soup', 'Chicken Soup', 'Vegetable Soup'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_recipes('chicken soup', recipes, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to experiment with different parameters with the model, as well as the bag of words contents. First, let's see if we can identify the \"best\" number of topics using KMeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_recipes('lentil', recipes, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_recipes('cheese pizza', recipes, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "docs_vec = vectorizer.fit_transform(docs_cleaned)\n",
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-54642b4182bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'k = {k}, silhouette score = {score}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    232\u001b[0m     reduce_func = functools.partial(_silhouette_reduce,\n\u001b[1;32m    233\u001b[0m                                     labels=labels, label_freqs=label_freqs)\n\u001b[0;32m--> 234\u001b[0;31m     results = zip(*pairwise_distances_chunked(X, reduce_func=reduce_func,\n\u001b[0m\u001b[1;32m    235\u001b[0m                                               **kwds))\n\u001b[1;32m    236\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minter_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[0m\u001b[1;32m   1624\u001b[0m                                      n_jobs=n_jobs, **kwds)\n\u001b[1;32m   1625\u001b[0m         if ((X is Y or Y is None)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    558\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    559\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mmajor_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# convert to this format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         idx_dtype = get_index_dtype((self.indptr, self.indices,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36masformat\u001b[0;34m(self, format, copy)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;31m# Forward the copy kwarg, if it's accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/scipy/sparse/csc.py\u001b[0m in \u001b[0;36mtocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         csc_tocsr(M, N,\n\u001b[0m\u001b[1;32m    143\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k=100\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(docs_vec)\n",
    "score = silhouette_score(docs_vec, kmeans.labels_)\n",
    "print(f'k = {k}, silhouette score = {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(kmeans, './models/kmeans_full.joblib')\n",
    "joblib.dump(vectorizer, './models/tf_vec_full.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = joblib.load('./models/kmeans_full.joblib')\n",
    "tf_vec = joblib.load('./models/tf_vec_full.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 features for each cluster.\n",
    "n_features = 10\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-(n_features+1):-1]\n",
    "print(\"top features (words) for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(f\"{num}, {', '.join(features[i] for i in centroid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random sample of texts in each cluster \\n\")\n",
    "assigned_cluster = kmeans.transform(docs_vec).argmin(axis=1)\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster = np.arange(0, docs_vec.shape[0])[assigned_cluster==i]\n",
    "    sample_recipes = np.random.choice(cluster, 10, replace=False)\n",
    "    \n",
    "    print(f'\\n cluster {i}:')\n",
    "    for idx in sample_recipes:\n",
    "        print(f'{recipes.iloc[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_count = []\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster = np.arange(0, docs_vec.shape[0])[assigned_cluster==i]\n",
    "    recipe_count.append(len(cluster))\n",
    "    print(f\"Cluster {i}: {len(cluster)} recipes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = []\n",
    "n_features = 1\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-(n_features+1):-1]\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    top_words.append(', '.join(features[i] for i in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "ax.set_title('Recipe Count By Cluster', fontsize = 24)\n",
    "ax.set_ylabel('Count', fontsize = 24)\n",
    "plt.yticks(fontsize = 20)\n",
    "ax.set_xticklabels(top_words, fontsize = 18, rotation = 90)\n",
    "ax.bar(top_words, recipe_count);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
